{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "324e8a16",
   "metadata": {},
   "source": [
    "# Selection of hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "183be9d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import ast\n",
    "import networkx as nx\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efd12ce1",
   "metadata": {},
   "source": [
    "# 1. Upload data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5d76f234",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_raw = pd.read_csv(\"train.csv\", sep=',')\n",
    "df_train_raw[\"edgelist\"].head(1)\n",
    "df_train_raw[\"edgelist\"] = df_train_raw[\"edgelist\"].apply(ast.literal_eval)\n",
    "\n",
    "df_test_raw = pd.read_csv(\"test.csv\", sep=',')\n",
    "df_test_raw[\"edgelist\"].head(1)\n",
    "df_test_raw[\"edgelist\"] = df_test_raw[\"edgelist\"].apply(ast.literal_eval)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e536ab68",
   "metadata": {},
   "source": [
    "# 2. Pre-Processing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a4dd84fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import numpy as np\n",
    "from itertools import combinations\n",
    "from community import community_louvain  # pip install python-louvain\n",
    "\n",
    "def normalize_group(df_group):\n",
    "    numeric_cols = [\n",
    "        'degree', 'closeness', 'betweenness', 'pagerank',\n",
    "        'eigenvector', 'katz', 'load',\n",
    "        'eccentricity', 'avg_neighbor_degree',\n",
    "         'community', 'is_leaf'\n",
    "        #'shortest_path_length', 'is_leaf', 'neighbor_connectivity'\n",
    "    ]\n",
    "    scaler = MinMaxScaler()\n",
    "    df_group[numeric_cols] = scaler.fit_transform(df_group[numeric_cols])\n",
    "    return df_group\n",
    "\n",
    "def pre_processing(data):\n",
    "    training_data = []\n",
    "\n",
    "    for idx, row in data.iterrows():\n",
    "        edgelist = row[\"edgelist\"]\n",
    "        \n",
    "        # Create undirected graph\n",
    "        T = nx.Graph()\n",
    "        T.add_edges_from(edgelist)\n",
    "\n",
    "        if not nx.is_connected(T):\n",
    "            continue\n",
    "        \n",
    "        root_node = row.get(\"root\", None)\n",
    "        \n",
    "        # Compute centralities\n",
    "        closeness = nx.closeness_centrality(T)\n",
    "        betweenness = nx.betweenness_centrality(T)\n",
    "        pagerank = nx.pagerank(T, max_iter=1000)\n",
    "        \n",
    "        # Additional centrality measures with fallbacks\n",
    "        try:\n",
    "            eigenvector = nx.eigenvector_centrality(T, max_iter=10000, tol=1e-06)\n",
    "        except nx.PowerIterationFailedConvergence:\n",
    "            eigenvector = {n: 0.0 for n in T.nodes}\n",
    "            \n",
    "        try:\n",
    "            katz = nx.katz_centrality(T, alpha=0.1)\n",
    "        except nx.NetworkXException:\n",
    "            katz = {n: 0.0 for n in T.nodes}\n",
    "            \n",
    "        try:\n",
    "            load = nx.load_centrality(T)\n",
    "        except:\n",
    "            load = {n: 0.0 for n in T.nodes}\n",
    "\n",
    "        # Structural properties\n",
    "        degree = dict(T.degree())\n",
    "        eccentricity = nx.eccentricity(T)\n",
    "        avg_neighbor_degree = nx.average_neighbor_degree(T)\n",
    "        \n",
    "        # Community detection\n",
    "        partition = community_louvain.best_partition(T)\n",
    "        \n",
    "        for v in T.nodes:\n",
    "            features = {\n",
    "                \"sentence\": row[\"sentence\"],\n",
    "                \"language\": row[\"language\"],\n",
    "                \"n\": row[\"n\"],\n",
    "                \"node\": v,\n",
    "\n",
    "                # Centrality measures\n",
    "                \"degree\": degree[v],\n",
    "                \"closeness\": closeness[v],\n",
    "                \"betweenness\": betweenness[v],\n",
    "                \"pagerank\": pagerank[v],\n",
    "                \"eigenvector\": eigenvector[v],\n",
    "                \"katz\": katz[v],\n",
    "                \"load\": load[v],\n",
    "\n",
    "                # Structural properties\n",
    "                \"eccentricity\": eccentricity[v],\n",
    "                \"avg_neighbor_degree\": avg_neighbor_degree[v],\n",
    "\n",
    "                # Community information\n",
    "                \"community\": partition[v],\n",
    "\n",
    "                \"is_leaf\": 1 if T.degree(v) == 1 else 0,\n",
    "            }\n",
    "\n",
    "            if \"id\" in row:\n",
    "                features[\"id\"] = row[\"id\"]\n",
    "\n",
    "            if root_node is not None:\n",
    "                features[\"is_root\"] = 1 if v == root_node else 0\n",
    "\n",
    "            training_data.append(features)\n",
    "\n",
    "    training_data = pd.DataFrame(training_data)\n",
    "    \n",
    "    # Normalize features by group\n",
    "    df_normalized = training_data.groupby([\"sentence\", \"language\"], group_keys=True).apply(\n",
    "        normalize_group, include_groups=False\n",
    "    )\n",
    "    df_normalized.reset_index(inplace=True)\n",
    "    df_normalized.drop(columns=[\"level_2\"], inplace=True)\n",
    "\n",
    "    return df_normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80041901",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pre_processing(df_train_raw)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e820604",
   "metadata": {},
   "source": [
    "# 3. Selection of hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bebbde95",
   "metadata": {},
   "outputs": [],
   "source": [
    "comparisons_rf = [\n",
    "    {\"max_depth\": 10, \"min_samples_split\": 5, \"n_estimators\": 50, \"class_weight\": \"balanced\"},  # árbol más pequeño\n",
    "    {\"max_depth\": 20, \"min_samples_split\": 10, \"n_estimators\": 50, \"class_weight\": \"balanced\"}, # más regularizado\n",
    "    {\"max_depth\": 20, \"min_samples_split\": 5, \"n_estimators\": 100, \"class_weight\": \"balanced\"}, # más árboles\n",
    "    {\"max_depth\": None, \"min_samples_split\": 5, \"n_estimators\": 50, \"class_weight\": \"balanced\"}, # sin límite de profundidad\n",
    "]\n",
    "\n",
    "\n",
    "comparisons_dt = [\n",
    "    {\"max_depth\": 50, \"min_samples_split\": 5, \"class_weight\": \"balanced\"},   # más limitado que el óptimo\n",
    "    {\"max_depth\": None, \"min_samples_split\": 10, \"class_weight\": \"balanced\"}, # menos sobreajuste\n",
    "    {\"max_depth\": 20, \"min_samples_split\": 5, \"class_weight\": \"balanced\"},    # muy limitado\n",
    "]\n",
    "\n",
    "comparisons_logreg_large = [\n",
    "    {\"penalty\": \"l2\", \"C\": 0.1, \"solver\": \"saga\", \"class_weight\": \"balanced\", \"max_iter\": 1000, \"n_jobs\": -1},  # strong regularization\n",
    "    {\"penalty\": \"l2\", \"C\": 1.0, \"solver\": \"saga\", \"class_weight\": \"balanced\", \"max_iter\": 1000, \"n_jobs\": -1},  # default\n",
    "    {\"penalty\": \"l1\", \"C\": 1.0, \"solver\": \"saga\", \"class_weight\": \"balanced\", \"max_iter\": 1000, \"n_jobs\": -1},  # sparse features\n",
    "    {\"penalty\": \"elasticnet\", \"C\": 1.0, \"solver\": \"saga\", \"l1_ratio\": 0.5, \"class_weight\": \"balanced\", \"max_iter\": 1000, \"n_jobs\": -1},  # mix L1/L2\n",
    "    {\"penalty\": \"l2\", \"C\": 0.1, \"solver\": \"lbfgs\", \"class_weight\": \"balanced\"},   # stronger regularization\n",
    "    {\"penalty\": \"l2\", \"C\": 1.0, \"solver\": \"lbfgs\", \"class_weight\": \"balanced\"},   # default regularization\n",
    "    {\"penalty\": \"l2\", \"C\": 10.0, \"solver\": \"lbfgs\", \"class_weight\": \"balanced\"}\n",
    "]\n",
    "\n",
    "\n",
    "features = [col for col in df_train.columns if col not in ['id', 'sentence', 'language', 'is_root', 'group_id']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af4cc696",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import StratifiedGroupKFold\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "\n",
    "# New class for Random Forest that takes into account group_id\n",
    "\n",
    "class GroupAwareRandomForest:\n",
    "    def __init__(self, n_estimators=50, max_depth=None, min_samples_split=2, class_weight=None, random_state=42):\n",
    "        self.n_estimators = n_estimators\n",
    "        self.max_depth = max_depth\n",
    "        self.min_samples_split = min_samples_split\n",
    "        self.class_weight = class_weight\n",
    "        self.random_state = random_state\n",
    "        self.trees = []\n",
    "\n",
    "    def fit(self, X, y, group_ids):\n",
    "        np.random.seed(self.random_state)\n",
    "        self.trees = []\n",
    "\n",
    "        unique_groups = np.unique(group_ids)\n",
    "        group_to_indices = defaultdict(list)\n",
    "        for idx, group in enumerate(group_ids):\n",
    "            group_to_indices[group].append(idx)\n",
    "\n",
    "        for _ in range(self.n_estimators):\n",
    "            sampled_groups = np.random.choice(unique_groups, size=int(0.8 * len(unique_groups)), replace=True)\n",
    "            sampled_indices = []\n",
    "            for group in sampled_groups:\n",
    "                sampled_indices.extend(group_to_indices[group])\n",
    "            sampled_indices = np.array(sampled_indices)\n",
    "\n",
    "            tree = DecisionTreeClassifier(\n",
    "                max_depth=self.max_depth,\n",
    "                min_samples_split=self.min_samples_split,\n",
    "                class_weight=self.class_weight,\n",
    "                random_state=self.random_state\n",
    "            )\n",
    "            tree.fit(X.iloc[sampled_indices], y.iloc[sampled_indices])\n",
    "            self.trees.append(tree)\n",
    "\n",
    "    def predict(self, X):\n",
    "        preds = np.zeros((len(self.trees), len(X)))\n",
    "        for i, tree in enumerate(self.trees):\n",
    "            preds[i] = tree.predict(X)\n",
    "        return (np.mean(preds, axis=0) > 0.5).astype(int)\n",
    "\n",
    "# Función de evaluación general\n",
    "def evaluate_models(df, features, comparisons_rf, comparisons_dt, comparisons_group_rf=None):\n",
    "    df['group_id'] = df[\"sentence\"].astype(str) + '_' + df[\"language\"]\n",
    "    X = df[features]\n",
    "    y = df['is_root']\n",
    "    groups = df[\"group_id\"]\n",
    "\n",
    "    results = []\n",
    "    cv = StratifiedGroupKFold(n_splits=10)\n",
    "\n",
    "    def evaluate_model(name, model, config, use_group_fit=False):\n",
    "        train_scores = []\n",
    "        val_scores = []\n",
    "\n",
    "        for train_idx, val_idx in cv.split(X, y, groups):\n",
    "            X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]\n",
    "            y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]\n",
    "            group_train = groups.iloc[train_idx]\n",
    "\n",
    "            if use_group_fit:\n",
    "                model.fit(X_train, y_train, group_train)\n",
    "            else:\n",
    "                model.fit(X_train, y_train)\n",
    "\n",
    "            y_train_pred = model.predict(X_train)\n",
    "            y_val_pred = model.predict(X_val)\n",
    "\n",
    "            f1_train = f1_score(y_train, y_train_pred, average='weighted')\n",
    "            f1_val = f1_score(y_val, y_val_pred, average='weighted')\n",
    "\n",
    "            train_scores.append(f1_train)\n",
    "            val_scores.append(f1_val)\n",
    "\n",
    "        avg_train = np.mean(train_scores)\n",
    "        avg_val = np.mean(val_scores)\n",
    "        std_train = np.std(train_scores)\n",
    "        std_val = np.std(val_scores)\n",
    "        avg_diff = np.mean(np.array(train_scores) - np.array(val_scores))\n",
    "\n",
    "        results.append({\n",
    "            \"Model\": name,\n",
    "            \"Params\": config,\n",
    "            \"Train_F1_mean\": round(avg_train, 4),\n",
    "            \"Train_F1_std\": round(std_train, 4),\n",
    "            \"Val_F1_mean\": round(avg_val, 4),\n",
    "            \"Val_F1_std\": round(std_val, 4),\n",
    "            \"Gap_Train-Val\": round(avg_diff, 4)\n",
    "        })\n",
    "\n",
    "    # Random Forest estándar\n",
    "    for cfg in comparisons_rf:\n",
    "        model = RandomForestClassifier(\n",
    "            max_depth=cfg[\"max_depth\"],\n",
    "            min_samples_split=cfg[\"min_samples_split\"],\n",
    "            n_estimators=cfg[\"n_estimators\"],\n",
    "            class_weight=cfg[\"class_weight\"],\n",
    "            random_state=42,\n",
    "            n_jobs=-1\n",
    "        )\n",
    "        evaluate_model(\"Random Forest\", model, cfg)\n",
    "\n",
    "    # Decision Tree estándar\n",
    "    for cfg in comparisons_dt:\n",
    "        model = DecisionTreeClassifier(\n",
    "            max_depth=cfg[\"max_depth\"],\n",
    "            min_samples_split=cfg[\"min_samples_split\"],\n",
    "            class_weight=cfg[\"class_weight\"],\n",
    "            random_state=42\n",
    "        )\n",
    "        evaluate_model(\"Decision Tree\", model, cfg)\n",
    "\n",
    "    # Random Forest con respeto a group_id\n",
    "    if comparisons_group_rf is not None:\n",
    "        for cfg in comparisons_group_rf:\n",
    "            model = GroupAwareRandomForest(\n",
    "                n_estimators=cfg[\"n_estimators\"],\n",
    "                max_depth=cfg[\"max_depth\"],\n",
    "                min_samples_split=cfg[\"min_samples_split\"],\n",
    "                class_weight=cfg[\"class_weight\"],\n",
    "                random_state=42\n",
    "            )\n",
    "            evaluate_model(\"GroupAware Random Forest\", model, cfg, use_group_fit=True)\n",
    "    for lr in comparisons_logreg_large:\n",
    "        model = LogisticRegression(\n",
    "            penalty=lr[\"penalty\"],\n",
    "            solver=lr[\"solver\"],\n",
    "            C=lr[\"C\"],\n",
    "            class_weight=\"balanced\",\n",
    "            max_iter=1000\n",
    "        )\n",
    "        evaluate_model(\"Logistic Regression\", model, lr)\n",
    "\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de2788b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = evaluate_models(\n",
    "    df_train,\n",
    "    features,\n",
    "    comparisons_rf,\n",
    "    comparisons_dt,\n",
    "    comparisons_rf\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d5596780",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(results).sort_values(by=\"Val_F1_mean\", ascending=False).to_csv(\"best_parameters.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
