{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "324e8a16",
   "metadata": {},
   "source": [
    "# ðŸ Machine Learning Project Script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "183be9d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import ast\n",
    "import networkx as nx\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efd12ce1",
   "metadata": {},
   "source": [
    "# 1. Upload data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "5d76f234",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_raw = pd.read_csv(\"train.csv\", sep=',')\n",
    "df_train_raw[\"edgelist\"].head(1)\n",
    "df_train_raw[\"edgelist\"] = df_train_raw[\"edgelist\"].apply(ast.literal_eval)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e536ab68",
   "metadata": {},
   "source": [
    "# 2. Pre-Processing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "a4dd84fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import numpy as np\n",
    "from itertools import combinations\n",
    "from community import community_louvain  # pip install python-louvain\n",
    "\n",
    "def normalize_group(df_group):\n",
    "    numeric_cols = [\n",
    "        'deg', 'degree', 'closeness', 'betweenness', 'pagerank',\n",
    "        'eigenvector', 'katz', 'harmonic', 'load',\n",
    "        'eccentricity', 'clustering', 'avg_neighbor_degree',\n",
    "        'triangles', 'community', 'square_clustering',\n",
    "        #'shortest_path_length', 'is_leaf', 'neighbor_connectivity'\n",
    "    ]\n",
    "    scaler = MinMaxScaler()\n",
    "    df_group[numeric_cols] = scaler.fit_transform(df_group[numeric_cols])\n",
    "    return df_group\n",
    "\n",
    "def pre_processing(data):\n",
    "    training_data = []\n",
    "\n",
    "    for idx, row in data.iterrows():\n",
    "        edgelist = row[\"edgelist\"]\n",
    "        \n",
    "        # Create undirected graph\n",
    "        T = nx.Graph()\n",
    "        T.add_edges_from(edgelist)\n",
    "\n",
    "        if not nx.is_connected(T):\n",
    "            continue\n",
    "        \n",
    "        # New root detection without directionality\n",
    "        deg_centrality = nx.degree_centrality(T)\n",
    "        #root = max(deg_centrality.items(), key=lambda x: x[1])[0]  # Most central node\n",
    "        \n",
    "        # Compute centralities\n",
    "        closeness = nx.closeness_centrality(T)\n",
    "        betweenness = nx.betweenness_centrality(T)\n",
    "        pagerank = nx.pagerank(T, max_iter=1000)\n",
    "        \n",
    "        # Additional centrality measures with fallbacks\n",
    "        try:\n",
    "            eigenvector = nx.eigenvector_centrality(T, max_iter=10000, tol=1e-06)\n",
    "        except nx.PowerIterationFailedConvergence:\n",
    "            eigenvector = {n: 0.0 for n in T.nodes}\n",
    "            \n",
    "        try:\n",
    "            katz = nx.katz_centrality(T, alpha=0.1)\n",
    "        except nx.NetworkXException:\n",
    "            katz = {n: 0.0 for n in T.nodes}\n",
    "            \n",
    "        harmonic = nx.harmonic_centrality(T)\n",
    "        try:\n",
    "            load = nx.load_centrality(T)\n",
    "        except:\n",
    "            load = {n: 0.0 for n in T.nodes}\n",
    "\n",
    "        # Structural properties\n",
    "        degree = dict(T.degree())\n",
    "        eccentricity = nx.eccentricity(T)\n",
    "        clustering = nx.clustering(T)\n",
    "        avg_neighbor_degree = nx.average_neighbor_degree(T)\n",
    "        triangles = nx.triangles(T)\n",
    "        square_clustering = nx.square_clustering(T)\n",
    "        \n",
    "        # Community detection\n",
    "        partition = community_louvain.best_partition(T)\n",
    "        \n",
    "        # New features\n",
    "        #shortest_path_length = nx.shortest_path_length(T, root)\n",
    "        \n",
    "        for v in T.nodes:\n",
    "            features = {\n",
    "                \"sentence\": row[\"sentence\"],\n",
    "                \"language\": row[\"language\"],\n",
    "                \"n\": row[\"n\"],\n",
    "                \"node\": v,\n",
    "                # Centrality measures\n",
    "                \"deg\": deg_centrality[v],\n",
    "                \"degree\": degree[v],\n",
    "                \"closeness\": closeness[v],\n",
    "                \"betweenness\": betweenness[v],\n",
    "                \"pagerank\": pagerank[v],\n",
    "                \"eigenvector\": eigenvector[v],\n",
    "                \"katz\": katz[v],\n",
    "                \"harmonic\": harmonic[v],\n",
    "                \"load\": load[v],\n",
    "                # Structural properties\n",
    "                \"eccentricity\": eccentricity[v],\n",
    "                \"clustering\": clustering[v],\n",
    "                \"avg_neighbor_degree\": avg_neighbor_degree[v],\n",
    "                \"triangles\": triangles[v],\n",
    "                \"square_clustering\": square_clustering[v],\n",
    "                # Community information\n",
    "                \"community\": partition[v],\n",
    "                # New features\n",
    "                #\"shortest_path_length\": shortest_path_length[v],\n",
    "                #\"is_leaf\": 1 if T.degree(v) == 1 else 0,\n",
    "                #\"neighbor_connectivity\": sum(1 for u in T.neighbors(v) for _ in nx.common_neighbors(T, v, u)),\n",
    "                # Target variable\n",
    "                #\"is_root\": 1 if v == root else 0\n",
    "            }\n",
    "            training_data.append(features)\n",
    "\n",
    "    training_data = pd.DataFrame(training_data)\n",
    "    \n",
    "    # Normalize features by group\n",
    "    df_normalized = training_data.groupby([\"sentence\", \"language\"], group_keys=True).apply(\n",
    "        normalize_group, include_groups=False\n",
    "    )\n",
    "    df_normalized.reset_index(inplace=True)\n",
    "    df_normalized.drop(columns=[\"level_2\"], inplace=True)\n",
    "\n",
    "    return df_normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "80041901",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pre_processing(df_train_raw)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e820604",
   "metadata": {},
   "source": [
    "# 3. Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "677bab87",
   "metadata": {},
   "source": [
    "**K-Fold Cross Validation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "678963a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedGroupKFold\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.metrics import precision_recall_fscore_support, recall_score, f1_score, classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "import xgboost as xgb\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from imblearn.ensemble import BalancedRandomForestClassifier\n",
    "\n",
    "def enhanced_training_pipeline(df, features, n_folds=5):\n",
    "    df['group_id'] = df[\"sentence\"].astype(str) + '_' + df[\"language\"]\n",
    "    X = df[features]\n",
    "    y = df['is_root']\n",
    "    groups = df[\"group_id\"]\n",
    "    \n",
    "    # Initialize models\n",
    "    models = {\n",
    "        \"Logistic Regression\": LogisticRegression(class_weight='balanced', max_iter=1000),\n",
    "        \"Random Forest\": RandomForestClassifier(class_weight='balanced', random_state=42),\n",
    "        \"Balanced RF\": BalancedRandomForestClassifier(random_state=42),\n",
    "        \"XGBoost\": XGBClassifier(use_label_encoder=False, eval_metric='logloss'),\n",
    "        \"Gradient Boosting\": GradientBoostingClassifier(random_state=42),\n",
    "    }\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    for model_name, model in models.items():\n",
    "        print(f\"\\n=== Evaluating {model_name} ===\")\n",
    "        fold_metrics = {'acc': [], 'prec': [], 'rec': [], 'f1': []}\n",
    "        simple_metrics = {'precision': [], 'recall': [], 'f1': [], 'support': []}\n",
    "        \n",
    "        cv = StratifiedGroupKFold(n_splits=n_folds)\n",
    "        \n",
    "        for fold, (train_idx, val_idx) in enumerate(cv.split(X, y, groups=groups)):\n",
    "            X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]\n",
    "            y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]\n",
    "            \n",
    "            # Handle class imbalance\n",
    "            if model_name not in [\"XGBoost\", \"Balanced RF\"]:\n",
    "                smote = SMOTE(sampling_strategy='minority', random_state=42)\n",
    "                X_train_res, y_train_res = smote.fit_resample(X_train, y_train)\n",
    "            else:\n",
    "                X_train_res, y_train_res = X_train, y_train\n",
    "                if model_name == \"XGBoost\":\n",
    "                    pos = y_train.sum()\n",
    "                    neg = len(y_train) - pos\n",
    "                    model.set_params(scale_pos_weight=neg/pos if pos > 0 else 1)\n",
    "            \n",
    "            # Feature selection for non-tree models\n",
    "            if model_name in [\"Logistic Regression\"]:\n",
    "                selector = SelectFromModel(\n",
    "                    estimator=LogisticRegression(max_iter=1000, class_weight='balanced'),\n",
    "                    max_features=15\n",
    "                ).fit(X_train_res, y_train_res)\n",
    "                X_train_fs = selector.transform(X_train_res)\n",
    "                X_val_fs = selector.transform(X_val)\n",
    "            else:\n",
    "                X_train_fs, X_val_fs = X_train_res, X_val\n",
    "            \n",
    "            # Fit model\n",
    "            model.fit(X_train_fs, y_train_res)\n",
    "            \n",
    "            # ===== Original Evaluation =====\n",
    "            #val_df = df.iloc[val_idx].copy()\n",
    "            #val_df['proba'] = model.predict_proba(X_val_fs)[:, 1]\n",
    "            \n",
    "            #predicted_roots = val_df.loc[val_df.groupby(['sentence', 'language'])['proba'].idxmax()]\n",
    "            #true_roots = val_df[val_df['is_root'] == 1]\n",
    "            \n",
    "            #merged = predicted_roots.merge(\n",
    "            #    true_roots, \n",
    "            #    on=['sentence', 'language'], \n",
    "            #    suffixes=('_pred', '_true')\n",
    "            #)\n",
    "            #\n",
    "            #correct = merged['node_pred'] == merged['node_true']\n",
    "            #acc = correct.mean()\n",
    "            #prec, rec, f1, _ = precision_recall_fscore_support(\n",
    "            #    correct, [True]*len(correct), average='binary'\n",
    "            #)\n",
    "            \n",
    "            #fold_metrics['acc'].append(acc)\n",
    "            #fold_metrics['prec'].append(prec)\n",
    "            #fold_metrics['rec'].append(rec)\n",
    "            #fold_metrics['f1'].append(f1)\n",
    "            \n",
    "            # ===== Simple Evaluation =====\n",
    "            y_pred = model.predict(X_val_fs)\n",
    "            report = classification_report(y_val, y_pred, output_dict=True)\n",
    "            simple_metrics['precision'].append(report['weighted avg']['precision'])\n",
    "            simple_metrics['recall'].append(report['weighted avg']['recall'])\n",
    "            simple_metrics['f1'].append(report['weighted avg']['f1-score'])\n",
    "            simple_metrics['support'].append(report['weighted avg']['support'])\n",
    "        \n",
    "        # Store results\n",
    "        results[model_name] = {\n",
    "            'simple': {\n",
    "                'precision': np.mean(simple_metrics['precision']),\n",
    "                'recall': np.mean(simple_metrics['recall']),\n",
    "                'f1': np.mean(simple_metrics['f1']),\n",
    "                'support': np.mean(simple_metrics['support'])\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        print(f\"\\nSimple Evaluation:\")\n",
    "        print(f\"Precision: {results[model_name]['simple']['precision']:.4f}\")\n",
    "        print(f\"Recall: {results[model_name]['simple']['recall']:.4f}\")\n",
    "        print(f\"F1: {results[model_name]['simple']['f1']:.4f}\")\n",
    "    \n",
    "    best_model_name = max(\n",
    "        [(name, res['simple']['f1']) for name, res in results.items()],\n",
    "        key=lambda x: x[1]\n",
    "    )[0]\n",
    "    \n",
    "    print(f\"\\nBest model: {best_model_name}\")\n",
    "    \n",
    "    best_model = models[best_model_name]\n",
    "    \n",
    "    if best_model_name not in [\"XGBoost\", \"Balanced RF\"]:\n",
    "        smote = SMOTE(sampling_strategy='minority', random_state=42)\n",
    "        X_res, y_res = smote.fit_resample(X, y)\n",
    "        best_model.fit(X_res, y_res)\n",
    "    else:\n",
    "        best_model.fit(X, y)\n",
    "    \n",
    "    return results, best_model_name, best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51fd11af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Evaluating Logistic Regression ===\n",
      "\n",
      "Simple Evaluation:\n",
      "Precision: 0.9772\n",
      "Recall: 0.9605\n",
      "F1: 0.9654\n",
      "\n",
      "=== Evaluating Random Forest ===\n",
      "\n",
      "Simple Evaluation:\n",
      "Precision: 0.9804\n",
      "Recall: 0.9759\n",
      "F1: 0.9774\n",
      "\n",
      "=== Evaluating Balanced RF ===\n",
      "\n",
      "Simple Evaluation:\n",
      "Precision: 0.9783\n",
      "Recall: 0.9641\n",
      "F1: 0.9682\n",
      "\n",
      "=== Evaluating XGBoost ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/apuiggro/.local/lib/python3.10/site-packages/xgboost/training.py:183: UserWarning: [12:45:40] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/apuiggro/.local/lib/python3.10/site-packages/xgboost/training.py:183: UserWarning: [12:45:41] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/apuiggro/.local/lib/python3.10/site-packages/xgboost/training.py:183: UserWarning: [12:45:42] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/apuiggro/.local/lib/python3.10/site-packages/xgboost/training.py:183: UserWarning: [12:45:43] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/apuiggro/.local/lib/python3.10/site-packages/xgboost/training.py:183: UserWarning: [12:45:43] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Simple Evaluation:\n",
      "Precision: 0.9798\n",
      "Recall: 0.9713\n",
      "F1: 0.9738\n",
      "\n",
      "=== Evaluating Gradient Boosting ===\n",
      "\n",
      "Simple Evaluation:\n",
      "Precision: 0.9788\n",
      "Recall: 0.9662\n",
      "F1: 0.9698\n",
      "\n",
      "Best model: Random Forest\n"
     ]
    }
   ],
   "source": [
    "# === Run pipeline ===\n",
    "\n",
    "features = [col for col in df_train.columns if col not in ['sentence', 'language', 'node', 'n', 'is_root', 'group_id']]\n",
    "\n",
    "results, best_model_name, best_model = enhanced_training_pipeline(df_train, features)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19f54ec8",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
