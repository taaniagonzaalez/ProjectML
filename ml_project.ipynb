{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "324e8a16",
   "metadata": {},
   "source": [
    "# üêç Machine Learning Project Script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "183be9d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import ast\n",
    "import networkx as nx\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efd12ce1",
   "metadata": {},
   "source": [
    "# 1. Upload data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5d76f234",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_raw = pd.read_csv(\"train.csv\", sep=',')\n",
    "df_train_raw[\"edgelist\"].head(1)\n",
    "df_train_raw[\"edgelist\"] = df_train_raw[\"edgelist\"].apply(ast.literal_eval)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e536ab68",
   "metadata": {},
   "source": [
    "# 2. Pre-Processing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "a4dd84fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Funci√≥n para normalizar por grupo\n",
    "def normalize_group(df_group):\n",
    "    #centrality_cols = ['deg', 'closeness', 'betweenness', 'pagerank', 'eigenvector', 'katz']\n",
    "    centrality_cols = ['deg', 'closeness', 'betweenness', 'pagerank']\n",
    "    scaler = MinMaxScaler()\n",
    "    df_group[centrality_cols] = scaler.fit_transform(df_group[centrality_cols])\n",
    "    return df_group\n",
    "\n",
    "def pre_processing(data):\n",
    "    # Container for the final dataset\n",
    "    training_data = []\n",
    "\n",
    "    # Iterate over each sentence\n",
    "    for idx, row in data.iterrows():\n",
    "        edgelist = row[\"edgelist\"]\n",
    "        \n",
    "        # Build tree\n",
    "        T = nx.from_edgelist(edgelist)\n",
    "        \n",
    "        # Skip disconnected graphs\n",
    "        if not nx.is_connected(T):\n",
    "            continue\n",
    "\n",
    "        # Assume root is the node that is never a child\n",
    "        children = set(v for _, v in edgelist)\n",
    "        parents = set(v for v, _ in edgelist)\n",
    "        root_candidates = list(parents - children)\n",
    "        root = root_candidates[0] if root_candidates else list(T.nodes)[0]  # fallback\n",
    "\n",
    "        # Compute centralities\n",
    "        deg_centrality = nx.degree_centrality(T)\n",
    "        closeness = nx.closeness_centrality(T)\n",
    "        betweenness = nx.betweenness_centrality(T)\n",
    "        pagerank = nx.pagerank(T, max_iter=1000)\n",
    "        eigenvector = nx.eigenvector_centrality(T, max_iter=10000, tol=1e-06)\n",
    "        katz = nx.katz_centrality(T, alpha=0.1)\n",
    "        \n",
    "        # Generate a row for each vertex\n",
    "        for v in T.nodes:\n",
    "            # features = {\n",
    "            #     \"sentence\": row[\"sentence\"],\n",
    "            #     \"language\": row[\"language\"],\n",
    "            #     \"n\": row[\"n\"],\n",
    "            #     \"node\": v,\n",
    "            #     \"deg\": deg_centrality[v],\n",
    "            #     \"closeness\": closeness[v],\n",
    "            #     \"betweenness\": betweenness[v],\n",
    "            #     \"pagerank\": pagerank[v],\n",
    "            #     \"eigenvector\" : eigenvector[v],\n",
    "            #     \"katz\" : katz[v],\n",
    "            #     \"is_root\": 1 if v == root else 0\n",
    "            # }\n",
    "            features = {\n",
    "                \"sentence\": row[\"sentence\"],\n",
    "                \"language\": row[\"language\"],\n",
    "                \"n\": row[\"n\"],\n",
    "                \"node\": v,\n",
    "                \"deg\": deg_centrality[v],\n",
    "                \"closeness\": closeness[v],\n",
    "                \"betweenness\": betweenness[v],\n",
    "                \"pagerank\": pagerank[v],\n",
    "                \"is_root\": 1 if v == root else 0\n",
    "            }\n",
    "            training_data.append(features)\n",
    "        \n",
    "    training_data = pd.DataFrame(training_data)\n",
    "\n",
    "    df_normalized = training_data.groupby([\"sentence\", \"language\"], group_keys=True).apply(normalize_group, include_groups=False)\n",
    "    df_normalized.reset_index(inplace=True)\n",
    "    df_normalized.drop(columns=[\"level_2\"], inplace=True)\n",
    "    return df_normalized\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "80041901",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pre_processing(df_train_raw)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e820604",
   "metadata": {},
   "source": [
    "# 3. Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "677bab87",
   "metadata": {},
   "source": [
    "**K-Fold Cross Validation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "43005dd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GroupKFold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "import numpy as np\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=RuntimeWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffcdd2aa",
   "metadata": {},
   "source": [
    "GKFolf_training allowing multiple roots per sentence (we don't want that)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "805d737a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def GKFold_training(model, features, df, n = 10):\n",
    "    gkf = GroupKFold(n_splits=n)\n",
    "\n",
    "    X = df[features]\n",
    "    Y = df['is_root']\n",
    "\n",
    "    df['group_id'] = df[\"sentence\"].astype(str) + '_' + df[\"language\"]\n",
    "\n",
    "    accs, precs, recalls, f1s = [], [], [], []\n",
    "\n",
    "    for fold, (train_idx, val_idx) in enumerate(gkf.split(X, Y, groups = df[\"group_id\"])):\n",
    "        X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]\n",
    "        y_train, y_val = Y.iloc[train_idx], Y.iloc[val_idx]\n",
    "\n",
    "        \n",
    "        model.fit(X_train, y_train)\n",
    "\n",
    "        y_pred = model.predict(X_val)\n",
    "\n",
    "        accs.append(accuracy_score(y_val, y_pred))\n",
    "        precs.append(precision_score(y_val, y_pred))\n",
    "        recalls.append(recall_score(y_val, y_pred))\n",
    "        f1s.append(f1_score(y_val, y_pred))\n",
    "    \n",
    "    print(f\"Average Accuracy:  {np.mean(accs):.4f}\")\n",
    "    print(f\"Average Precision: {np.mean(precs):.4f}\")\n",
    "    print(f\"Average Recall:    {np.mean(recalls):.4f}\")\n",
    "    print(f\"Average F1 Score:  {np.mean(f1s):.4f}\")\n",
    "    \n",
    "    return np.mean(accs), np.mean(precs), np.mean(recalls), np.mean(f1s)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99118521",
   "metadata": {},
   "source": [
    "GKFolf_training not allowing multiple roots per sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "bf4c0855",
   "metadata": {},
   "outputs": [],
   "source": [
    "def GKFold_training(model, features, df, n=10):\n",
    "    gkf = GroupKFold(n_splits=n)\n",
    "\n",
    "    X = df[features]\n",
    "    Y = df['is_root']\n",
    "    df['group_id'] = df[\"sentence\"].astype(str) + '_' + df[\"language\"]\n",
    "\n",
    "    accs, precs, recalls, f1s = [], [], [], []\n",
    "\n",
    "    for fold, (train_idx, val_idx) in enumerate(gkf.split(X, Y, groups=df[\"group_id\"])):\n",
    "        # Training\n",
    "        X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]\n",
    "        y_train, y_val = Y.iloc[train_idx], Y.iloc[val_idx]\n",
    "\n",
    "        model.fit(X_train, y_train)\n",
    "\n",
    "        # Predict probabilities\n",
    "        val_df = df.iloc[val_idx].copy()\n",
    "        val_df['proba'] = model.predict_proba(X_val)[:, 1]\n",
    "\n",
    "        # Select node with highest proba per (sentence, language)\n",
    "        predicted_roots = val_df.loc[val_df.groupby(['sentence', 'language'])['proba'].idxmax()]\n",
    "\n",
    "        # True root per tree = where is_root == 1\n",
    "        true_roots = val_df[val_df['is_root'] == 1]\n",
    "        merged = predicted_roots.merge(true_roots, on=['sentence', 'language'], suffixes=('_pred', '_true'))\n",
    "\n",
    "        # Compare predicted node to true root\n",
    "        correct = merged['node_pred'] == merged['node_true']\n",
    "        acc = correct.mean()\n",
    "\n",
    "        # Classification metrics at sentence level\n",
    "        precision = precision_score(correct, [True] * len(correct))\n",
    "        recall = recall_score(correct, [True] * len(correct))\n",
    "        f1 = f1_score(correct, [True] * len(correct))\n",
    "\n",
    "        accs.append(acc)\n",
    "        precs.append(precision)\n",
    "        recalls.append(recall)\n",
    "        f1s.append(f1)\n",
    "\n",
    "    print(f\"Average Accuracy:  {np.mean(accs):.4f}\")\n",
    "    print(f\"Average Precision: {np.mean(precs):.4f}\")\n",
    "    print(f\"Average Recall:    {np.mean(recalls):.4f}\")\n",
    "    print(f\"Average F1 Score:  {np.mean(f1s):.4f}\")\n",
    "\n",
    "    return np.mean(accs), np.mean(precs), np.mean(recalls), np.mean(f1s)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6e0270d",
   "metadata": {},
   "source": [
    "## Logistic Regression\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "e73c5c31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Accuracy:  0.2695\n",
      "Average Precision: 0.2695\n",
      "Average Recall:    1.0000\n",
      "Average F1 Score:  0.4244\n"
     ]
    }
   ],
   "source": [
    "model_lr = LogisticRegression(class_weight='balanced')\n",
    "# model_lr_adv = LogisticRegression(penalty = 'l2', \n",
    "#                                   solver = 'saga',   # Sparse features: have mostly 0-values\n",
    "#                                   C=1.0, \n",
    "#                                   class_weight='balanced',\n",
    "#                                   max_iter=1000,\n",
    "#                                   random_state=42)\n",
    "\n",
    "#features = ['deg', 'closeness', 'betweenness', 'pagerank']\n",
    "features_n = ['deg', 'closeness', 'betweenness', 'pagerank', 'n']\n",
    "#features_add = ['deg', 'closeness', 'betweenness', 'pagerank', 'n','eigenvector', 'katz']\n",
    "\n",
    "#acc_lr, prec_lr, recall_lr, f1_lr = GKFold_training(model_lr, features, df_normalized)\n",
    "acc_lr, prec_lr, recall_lr, f1_lr = GKFold_training(model_lr, features_n, df_train)\n",
    "#acc_lr, prec_lr, recall_lr, f1_lr = GKFold_training(model_lr_adv, features_n, df_normalized)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19f54ec8",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b36e2dbe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
