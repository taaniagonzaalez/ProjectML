{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "324e8a16",
   "metadata": {},
   "source": [
    "# 🐍 Machine Learning Project Script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "183be9d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import ast\n",
    "import networkx as nx\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efd12ce1",
   "metadata": {},
   "source": [
    "# 1. Upload data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5d76f234",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_raw = pd.read_csv(\"train.csv\", sep=',')\n",
    "df_train_raw[\"edgelist\"].head(1)\n",
    "df_train_raw[\"edgelist\"] = df_train_raw[\"edgelist\"].apply(ast.literal_eval)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e536ab68",
   "metadata": {},
   "source": [
    "# 2. Pre-Processing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a4dd84fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "def normalize_group(df_group):\n",
    "    # Normalize all numeric centrality and graph metrics\n",
    "    numeric_cols = [\n",
    "        'deg', 'degree', 'closeness', 'betweenness', 'pagerank',\n",
    "        'eigenvector', 'katz', 'eccentricity', 'clustering', 'distance_to_root'\n",
    "    ]\n",
    "    scaler = MinMaxScaler()\n",
    "    df_group[numeric_cols] = scaler.fit_transform(df_group[numeric_cols])\n",
    "    return df_group\n",
    "\n",
    "def pre_processing(data):\n",
    "    training_data = []\n",
    "\n",
    "    for idx, row in data.iterrows():\n",
    "        edgelist = row[\"edgelist\"]\n",
    "\n",
    "        T = nx.from_edgelist(edgelist)\n",
    "\n",
    "        # Skip disconnected graphs\n",
    "        if not nx.is_connected(T):\n",
    "            continue\n",
    "\n",
    "        children = set(v for _, v in edgelist)\n",
    "        parents = set(v for v, _ in edgelist)\n",
    "        root_candidates = list(parents - children)\n",
    "        root = root_candidates[0] if root_candidates else list(T.nodes)[0]\n",
    "\n",
    "        # Compute centralities\n",
    "        deg_centrality = nx.degree_centrality(T)\n",
    "        closeness = nx.closeness_centrality(T)\n",
    "        betweenness = nx.betweenness_centrality(T)\n",
    "        pagerank = nx.pagerank(T, max_iter=1000)\n",
    "        try:\n",
    "            eigenvector = nx.eigenvector_centrality(T, max_iter=10000, tol=1e-06)\n",
    "        except nx.PowerIterationFailedConvergence:\n",
    "            eigenvector = {n: 0.0 for n in T.nodes}\n",
    "        try:\n",
    "            katz = nx.katz_centrality(T, alpha=0.1)\n",
    "        except nx.NetworkXException:\n",
    "            katz = {n: 0.0 for n in T.nodes}\n",
    "\n",
    "        # Additional graph properties\n",
    "        degree = dict(T.degree())\n",
    "        eccentricity = nx.eccentricity(T)\n",
    "        clustering = nx.clustering(T)\n",
    "        shortest_paths = nx.shortest_path_length(T, source=root)\n",
    "\n",
    "        for v in T.nodes:\n",
    "            features = {\n",
    "                \"sentence\": row[\"sentence\"],\n",
    "                \"language\": row[\"language\"],\n",
    "                \"n\": row[\"n\"],\n",
    "                \"node\": v,\n",
    "                \"deg\": deg_centrality[v],\n",
    "                \"degree\": degree[v],\n",
    "                \"closeness\": closeness[v],\n",
    "                \"betweenness\": betweenness[v],\n",
    "                \"pagerank\": pagerank[v],\n",
    "                \"eigenvector\": eigenvector[v],\n",
    "                \"katz\": katz[v],\n",
    "                \"eccentricity\": eccentricity[v],\n",
    "                \"clustering\": clustering[v],\n",
    "                \"distance_to_root\": shortest_paths[v],\n",
    "                \"is_root\": 1 if v == root else 0\n",
    "            }\n",
    "            training_data.append(features)\n",
    "\n",
    "    training_data = pd.DataFrame(training_data)\n",
    "\n",
    "    # Normalize the numeric features by group\n",
    "    df_normalized = training_data.groupby([\"sentence\", \"language\"], group_keys=True).apply(\n",
    "        normalize_group, include_groups=False\n",
    "    )\n",
    "    df_normalized.reset_index(inplace=True)\n",
    "    df_normalized.drop(columns=[\"level_2\"], inplace=True)\n",
    "\n",
    "    return df_normalized\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "80041901",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m df_train \u001b[38;5;241m=\u001b[39m \u001b[43mpre_processing\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf_train_raw\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[5], line 42\u001b[0m, in \u001b[0;36mpre_processing\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m     40\u001b[0m     eigenvector \u001b[38;5;241m=\u001b[39m {n: \u001b[38;5;241m0.0\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m n \u001b[38;5;129;01min\u001b[39;00m T\u001b[38;5;241m.\u001b[39mnodes}\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 42\u001b[0m     katz \u001b[38;5;241m=\u001b[39m \u001b[43mnx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkatz_centrality\u001b[49m\u001b[43m(\u001b[49m\u001b[43mT\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43malpha\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m nx\u001b[38;5;241m.\u001b[39mNetworkXException:\n\u001b[1;32m     44\u001b[0m     katz \u001b[38;5;241m=\u001b[39m {n: \u001b[38;5;241m0.0\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m n \u001b[38;5;129;01min\u001b[39;00m T\u001b[38;5;241m.\u001b[39mnodes}\n",
      "File \u001b[0;32m<class 'networkx.utils.decorators.argmap'> compilation 52:4\u001b[0m, in \u001b[0;36margmap_katz_centrality_48\u001b[0;34m(G, alpha, beta, max_iter, tol, nstart, normalized, weight, backend, **backend_kwargs)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mcollections\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mgzip\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01minspect\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mitertools\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mre\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/networkx/utils/backends.py:967\u001b[0m, in \u001b[0;36m_dispatchable.__call__\u001b[0;34m(self, backend, *args, **kwargs)\u001b[0m\n\u001b[1;32m    965\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m backend \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m backend \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnetworkx\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    966\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbackend\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m backend is not installed\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 967\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43morig_func\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    969\u001b[0m \u001b[38;5;66;03m# Use `backend_name` in this function instead of `backend`.\u001b[39;00m\n\u001b[1;32m    970\u001b[0m \u001b[38;5;66;03m# This is purely for aesthetics and to make it easier to search for this\u001b[39;00m\n\u001b[1;32m    971\u001b[0m \u001b[38;5;66;03m# variable since \"backend\" is used in many comments and log/error messages.\u001b[39;00m\n\u001b[1;32m    972\u001b[0m backend_name \u001b[38;5;241m=\u001b[39m backend\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/networkx/algorithms/centrality/katz.py:176\u001b[0m, in \u001b[0;36mkatz_centrality\u001b[0;34m(G, alpha, beta, max_iter, tol, nstart, normalized, weight)\u001b[0m\n\u001b[1;32m    174\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m n \u001b[38;5;129;01min\u001b[39;00m x:\n\u001b[1;32m    175\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m nbr \u001b[38;5;129;01min\u001b[39;00m G[n]:\n\u001b[0;32m--> 176\u001b[0m         x[nbr] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m xlast[n] \u001b[38;5;241m*\u001b[39m \u001b[43mG\u001b[49m\u001b[43m[\u001b[49m\u001b[43mn\u001b[49m\u001b[43m]\u001b[49m[nbr]\u001b[38;5;241m.\u001b[39mget(weight, \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    177\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m n \u001b[38;5;129;01min\u001b[39;00m x:\n\u001b[1;32m    178\u001b[0m     x[n] \u001b[38;5;241m=\u001b[39m alpha \u001b[38;5;241m*\u001b[39m x[n] \u001b[38;5;241m+\u001b[39m b[n]\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/networkx/classes/graph.py:498\u001b[0m, in \u001b[0;36mGraph.__getitem__\u001b[0;34m(self, n)\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Returns the number of nodes in the graph. Use: 'len(G)'.\u001b[39;00m\n\u001b[1;32m    478\u001b[0m \n\u001b[1;32m    479\u001b[0m \u001b[38;5;124;03m    Returns\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    494\u001b[0m \n\u001b[1;32m    495\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m    496\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_node)\n\u001b[0;32m--> 498\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, n):\n\u001b[1;32m    499\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Returns a dict of neighbors of node n.  Use: 'G[n]'.\u001b[39;00m\n\u001b[1;32m    500\u001b[0m \n\u001b[1;32m    501\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    520\u001b[0m \u001b[38;5;124;03m    AtlasView({1: {}})\u001b[39;00m\n\u001b[1;32m    521\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m    522\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madj[n]\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "df_train = pre_processing(df_train_raw)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e820604",
   "metadata": {},
   "source": [
    "# 3. Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "677bab87",
   "metadata": {},
   "source": [
    "**K-Fold Cross Validation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "678963a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import GroupKFold\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.utils.class_weight import compute_sample_weight\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "import matplotlib.pyplot as plt\n",
    "import xgboost as xgb\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# You may want to fix model key in MLPClassifier entry:\n",
    "models = {\n",
    "    \"Logistic Regression\": LogisticRegression(class_weight='balanced', max_iter=1000),\n",
    "    \"Random Forest\": RandomForestClassifier(class_weight='balanced', random_state=42),\n",
    "    \"Gradient Boosting\": GradientBoostingClassifier(random_state=42),\n",
    "    \"XGBoost\": XGBClassifier(scale_pos_weight=None, use_label_encoder=False, eval_metric='logloss'),\n",
    "    \"SVM\": SVC(class_weight='balanced', probability=True, kernel='rbf'),\n",
    "    \"Neural Network\": MLPClassifier(hidden_layer_sizes=(64, 32), early_stopping=True)\n",
    "}\n",
    "\n",
    "def enhanced_training_pipeline(df, features, n_folds=10):\n",
    "    # Create group_id for grouping by sentence and language (adapt column names if needed)\n",
    "    df['group_id'] = df[\"sentence\"].astype(str) + '_' + df[\"language\"]\n",
    "\n",
    "    X = df[features]\n",
    "    y = df['is_root']\n",
    "    groups = df[\"group_id\"]\n",
    "\n",
    "    # Compute scale_pos_weight dynamically for XGBoost inside loop per fold (better for imbalance)\n",
    "    results = {}\n",
    "\n",
    "    for model_name, model in models.items():\n",
    "        print(f\"\\n=== Evaluating {model_name} ===\")\n",
    "        fold_metrics = {'acc': [], 'prec': [], 'rec': [], 'f1': []}\n",
    "\n",
    "        for fold, (train_idx, val_idx) in enumerate(GroupKFold(n_splits=n_folds).split(X, y, groups=groups)):\n",
    "            X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]\n",
    "            y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]\n",
    "\n",
    "            # For XGBoost update scale_pos_weight based on training fold\n",
    "            if model_name == \"XGBoost\":\n",
    "                pos = y_train.sum()\n",
    "                neg = len(y_train) - pos\n",
    "                model.set_params(scale_pos_weight=neg / pos if pos > 0 else 1)\n",
    "                X_train_fs, X_val_fs = X_train, X_val\n",
    "            else:\n",
    "                # Feature selection with logistic regression for other models\n",
    "                max_feats = min(15, X_train.shape[1])\n",
    "                selector = SelectFromModel(\n",
    "                    estimator=LogisticRegression(max_iter=1000, class_weight='balanced'),\n",
    "                    max_features=max_feats\n",
    "                ).fit(X_train, y_train)\n",
    "                X_train_fs = selector.transform(X_train)\n",
    "                X_val_fs = selector.transform(X_val)\n",
    "\n",
    "            sample_weights = compute_sample_weight(\n",
    "                class_weight='balanced',\n",
    "                y=y_train\n",
    "            )\n",
    "\n",
    "            # Fit model\n",
    "            if model_name == \"Neural Network\":\n",
    "                model.fit(X_train_fs, y_train)\n",
    "            else:\n",
    "                model.fit(X_train_fs, y_train, sample_weight=sample_weights)\n",
    "\n",
    "            val_df = df.iloc[val_idx].copy()\n",
    "            val_df['proba'] = model.predict_proba(X_val_fs)[:, 1]\n",
    "\n",
    "            # For each sentence-language group, pick node with highest probability as predicted root\n",
    "            predicted_roots = val_df.loc[val_df.groupby(['sentence', 'language'])['proba'].idxmax()]\n",
    "            true_roots = val_df[val_df['is_root'] == 1]\n",
    "\n",
    "            # Merge predicted roots with true roots by group\n",
    "            merged = predicted_roots.merge(\n",
    "                true_roots, \n",
    "                on=['sentence', 'language'], \n",
    "                suffixes=('_pred', '_true')\n",
    "            )\n",
    "\n",
    "            # Correct prediction if predicted node matches true root node\n",
    "            correct = merged['node_pred'] == merged['node_true']\n",
    "            acc = correct.mean()\n",
    "\n",
    "            fold_metrics['acc'].append(acc)\n",
    "            fold_metrics['prec'].append(precision_score(correct, [True]*len(correct)))\n",
    "            fold_metrics['rec'].append(recall_score(correct, [True]*len(correct)))\n",
    "            fold_metrics['f1'].append(f1_score(correct, [True]*len(correct)))\n",
    "\n",
    "        results[model_name] = {\n",
    "            'accuracy': np.mean(fold_metrics['acc']),\n",
    "            'precision': np.mean(fold_metrics['prec']),\n",
    "            'recall': np.mean(fold_metrics['rec']),\n",
    "            'f1': np.mean(fold_metrics['f1'])\n",
    "        }\n",
    "\n",
    "        print(f\"Average Accuracy: {results[model_name]['accuracy']:.4f}\")\n",
    "        print(f\"Average F1: {results[model_name]['f1']:.4f}\")\n",
    "\n",
    "    best_model_name = max(results, key=lambda x: results[x]['f1'])\n",
    "    print(f\"\\nBest model: {best_model_name}\")\n",
    "\n",
    "    # Fit best model on full data\n",
    "    best_model = models[best_model_name]\n",
    "\n",
    "    if best_model_name == \"XGBoost\":\n",
    "        pos = y.sum()\n",
    "        neg = len(y) - pos\n",
    "        best_model.set_params(scale_pos_weight=neg / pos if pos > 0 else 1)\n",
    "\n",
    "    best_model.fit(X[features], y)\n",
    "\n",
    "    if best_model_name == \"XGBoost\":\n",
    "        plt.figure(figsize=(10, 8))\n",
    "        xgb.plot_importance(best_model, max_num_features=15)\n",
    "        plt.title('Feature Importance')\n",
    "        plt.show()\n",
    "\n",
    "    return results, best_model_name, best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51fd11af",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# === Run pipeline ===\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m features \u001b[38;5;241m=\u001b[39m [col \u001b[38;5;28;01mfor\u001b[39;00m col \u001b[38;5;129;01min\u001b[39;00m \u001b[43mdf_train\u001b[49m\u001b[38;5;241m.\u001b[39mcolumns \u001b[38;5;28;01mif\u001b[39;00m col \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msentence\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlanguage\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnode\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mn\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mis_root\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgroup_id\u001b[39m\u001b[38;5;124m'\u001b[39m]]\n\u001b[1;32m      5\u001b[0m results, best_model_name, best_model \u001b[38;5;241m=\u001b[39m enhanced_training_pipeline(df_train, features)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df_train' is not defined"
     ]
    }
   ],
   "source": [
    "# === Run pipeline ===\n",
    "\n",
    "features = [col for col in df_train.columns if col not in ['sentence', 'language', 'node', 'n', 'is_root', 'group_id']]\n",
    "\n",
    "results, best_model_name, best_model = enhanced_training_pipeline(df_train, features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "812fd29b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['deg', 'degree', 'closeness', 'betweenness', 'pagerank', 'eigenvector', 'katz', 'eccentricity', 'clustering', 'distance_to_root', 'group_id']\n",
      "deg                 float64\n",
      "degree              float64\n",
      "closeness           float64\n",
      "betweenness         float64\n",
      "pagerank            float64\n",
      "eigenvector         float64\n",
      "katz                float64\n",
      "eccentricity        float64\n",
      "clustering          float64\n",
      "distance_to_root    float64\n",
      "group_id             object\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(features)\n",
    "print(df_train[features].dtypes)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "805d737a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross-validation training function\n",
    "def enhanced_GKFold_training(model, features, df, n=10):\n",
    "    gkf = GroupKFold(n_splits=n)\n",
    "    df['group_id'] = df[\"sentence\"].astype(str) + '_' + df[\"language\"]\n",
    "    \n",
    "    metrics = {\n",
    "        'accuracy': [],\n",
    "        'precision': [],\n",
    "        'recall': [],\n",
    "        'f1': [],\n",
    "        'roc_auc': []\n",
    "    }\n",
    "    \n",
    "    df['proba'] = 0  # Initialize column for storing probabilities\n",
    "    \n",
    "    for train_idx, val_idx in gkf.split(df, df['is_root'], groups=df[\"group_id\"]):\n",
    "        # Prepare data\n",
    "        X_train = df.iloc[train_idx][features]\n",
    "        y_train = df.iloc[train_idx]['is_root']\n",
    "        X_val = df.iloc[val_idx][features]\n",
    "        y_val = df.iloc[val_idx]['is_root']\n",
    "        \n",
    "        # Train and predict\n",
    "        model.fit(X_train, y_train)\n",
    "        y_proba = model.predict_proba(X_val)[:, 1]\n",
    "        \n",
    "        # Store probabilities for tree-level evaluation\n",
    "        df.loc[val_idx, 'proba'] = y_proba\n",
    "        \n",
    "        # Node-level metrics\n",
    "        y_pred = model.predict(X_val)\n",
    "        metrics['accuracy'].append(accuracy_score(y_val, y_pred))\n",
    "        metrics['precision'].append(precision_score(y_val, y_pred, zero_division=0))\n",
    "        metrics['recall'].append(recall_score(y_val, y_pred))\n",
    "        metrics['f1'].append(f1_score(y_val, y_pred))\n",
    "        metrics['roc_auc'].append(roc_auc_score(y_val, y_proba))\n",
    "    \n",
    "    # Tree-level evaluation\n",
    "    predicted_roots = df.loc[df.groupby(['sentence', 'language'])['proba'].idxmax()]\n",
    "    true_roots = df[df['is_root'] == 1]\n",
    "    merged = predicted_roots.merge(true_roots, on=['sentence', 'language'], \n",
    "                                   suffixes=('_pred', '_true'))\n",
    "    tree_accuracy = (merged['node_pred'] == merged['node_true']).mean()\n",
    "    \n",
    "    # Print results\n",
    "    print(\"Node-Level Metrics:\")\n",
    "    for metric, values in metrics.items():\n",
    "        print(f\"{metric.capitalize()}: {np.mean(values):.4f} ± {np.std(values):.4f}\")\n",
    "    \n",
    "    print(f\"\\nTree-Level Accuracy: {tree_accuracy:.4f}\")\n",
    "    \n",
    "    return metrics, tree_accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e08fa54b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Training Logistic Regression ===\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "\"['deg', 'closeness', 'betweenness', 'n'] not in index\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[27], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m name, model \u001b[38;5;129;01min\u001b[39;00m models\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m=== Training \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m ===\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 6\u001b[0m     metrics, tree_acc \u001b[38;5;241m=\u001b[39m \u001b[43menhanced_GKFold_training\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeatures_n\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdf_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m     all_metrics[name] \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m      8\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmetrics\u001b[39m\u001b[38;5;124m\"\u001b[39m: metrics,\n\u001b[1;32m      9\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtree_accuracy\u001b[39m\u001b[38;5;124m\"\u001b[39m: tree_acc\n\u001b[1;32m     10\u001b[0m     }\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# Optional: Plot performance\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[23], line 15\u001b[0m, in \u001b[0;36menhanced_GKFold_training\u001b[0;34m(model, features, df, n)\u001b[0m\n\u001b[1;32m      5\u001b[0m metrics \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m: [],\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprecision\u001b[39m\u001b[38;5;124m'\u001b[39m: [],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mroc_auc\u001b[39m\u001b[38;5;124m'\u001b[39m: []\n\u001b[1;32m     11\u001b[0m }\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m train_idx, val_idx \u001b[38;5;129;01min\u001b[39;00m gkf\u001b[38;5;241m.\u001b[39msplit(df, df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mis_root\u001b[39m\u001b[38;5;124m'\u001b[39m], groups\u001b[38;5;241m=\u001b[39mdf[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgroup_id\u001b[39m\u001b[38;5;124m\"\u001b[39m]):\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;66;03m# Prepare data\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m     X_train \u001b[38;5;241m=\u001b[39m \u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miloc\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtrain_idx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     16\u001b[0m     y_train \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39miloc[train_idx][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mis_root\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     17\u001b[0m     X_val \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39miloc[val_idx][features]\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/core/frame.py:4108\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   4106\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_iterator(key):\n\u001b[1;32m   4107\u001b[0m         key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(key)\n\u001b[0;32m-> 4108\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_indexer_strict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcolumns\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m   4110\u001b[0m \u001b[38;5;66;03m# take() does not accept boolean indexers\u001b[39;00m\n\u001b[1;32m   4111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(indexer, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mbool\u001b[39m:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/core/indexes/base.py:6200\u001b[0m, in \u001b[0;36mIndex._get_indexer_strict\u001b[0;34m(self, key, axis_name)\u001b[0m\n\u001b[1;32m   6197\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   6198\u001b[0m     keyarr, indexer, new_indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reindex_non_unique(keyarr)\n\u001b[0;32m-> 6200\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_raise_if_missing\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkeyarr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   6202\u001b[0m keyarr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtake(indexer)\n\u001b[1;32m   6203\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, Index):\n\u001b[1;32m   6204\u001b[0m     \u001b[38;5;66;03m# GH 42790 - Preserve name from an Index\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/core/indexes/base.py:6252\u001b[0m, in \u001b[0;36mIndex._raise_if_missing\u001b[0;34m(self, key, indexer, axis_name)\u001b[0m\n\u001b[1;32m   6249\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNone of [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m] are in the [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00maxis_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   6251\u001b[0m not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(ensure_index(key)[missing_mask\u001b[38;5;241m.\u001b[39mnonzero()[\u001b[38;5;241m0\u001b[39m]]\u001b[38;5;241m.\u001b[39munique())\n\u001b[0;32m-> 6252\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnot_found\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not in index\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mKeyError\u001b[0m: \"['deg', 'closeness', 'betweenness', 'n'] not in index\""
     ]
    }
   ],
   "source": [
    "# Train all models and store results\n",
    "all_metrics = {}\n",
    "\n",
    "for name, model in models.items():\n",
    "    print(f\"\\n=== Training {name} ===\")\n",
    "    metrics, tree_acc = enhanced_GKFold_training(model, features_n, df_train)\n",
    "    all_metrics[name] = {\n",
    "        \"metrics\": metrics,\n",
    "        \"tree_accuracy\": tree_acc\n",
    "    }\n",
    "\n",
    "# Optional: Plot performance\n",
    "f1_scores = [np.mean(m['metrics']['f1']) for m in all_metrics.values()]\n",
    "tree_accs = [m['tree_accuracy'] for m in all_metrics.values()]\n",
    "model_names = list(all_metrics.keys())\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.barh(model_names, f1_scores, color='skyblue')\n",
    "plt.title(\"Average F1 Score\")\n",
    "plt.xlabel(\"F1 Score\")\n",
    "plt.xlim(0, 1)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.barh(model_names, tree_accs, color='orange')\n",
    "plt.title(\"Tree-Level Accuracy\")\n",
    "plt.xlabel(\"Accuracy\")\n",
    "plt.xlim(0, 1)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa17324b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "ensemble = VotingClassifier(\n",
    "    estimators=[\n",
    "        ('lr', LogisticRegression(class_weight='balanced')),\n",
    "        ('rf', RandomForestClassifier(class_weight='balanced')),\n",
    "        ('xgb', XGBClassifier(scale_pos_weight=...))\n",
    "    ],\n",
    "    voting='soft'  # Use probabilities\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6e0270d",
   "metadata": {},
   "source": [
    "## Logistic Regression\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e73c5c31",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"['deg', 'closeness', 'betweenness', 'n'] not in index\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[26], line 14\u001b[0m\n\u001b[1;32m     10\u001b[0m features_n \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdeg\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcloseness\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbetweenness\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpagerank\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mn\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m#features_add = ['deg', 'closeness', 'betweenness', 'pagerank', 'n','eigenvector', 'katz']\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m#acc_lr, prec_lr, recall_lr, f1_lr = GKFold_training(model_lr, features, df_normalized)\u001b[39;00m\n\u001b[0;32m---> 14\u001b[0m acc_lr, prec_lr, recall_lr, f1_lr \u001b[38;5;241m=\u001b[39m \u001b[43mGKFold_training\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_lr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeatures_n\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdf_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m#acc_lr, prec_lr, recall_lr, f1_lr = GKFold_training(model_lr_adv, features_n, df_normalized)\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[25], line 4\u001b[0m, in \u001b[0;36mGKFold_training\u001b[0;34m(model, features, df, n)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mGKFold_training\u001b[39m(model, features, df, n\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m):\n\u001b[1;32m      2\u001b[0m     gkf \u001b[38;5;241m=\u001b[39m GroupKFold(n_splits\u001b[38;5;241m=\u001b[39mn)\n\u001b[0;32m----> 4\u001b[0m     X \u001b[38;5;241m=\u001b[39m \u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m      5\u001b[0m     Y \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mis_root\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m      6\u001b[0m     df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgroup_id\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msentence\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlanguage\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/core/frame.py:4108\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   4106\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_iterator(key):\n\u001b[1;32m   4107\u001b[0m         key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(key)\n\u001b[0;32m-> 4108\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_indexer_strict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcolumns\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m   4110\u001b[0m \u001b[38;5;66;03m# take() does not accept boolean indexers\u001b[39;00m\n\u001b[1;32m   4111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(indexer, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mbool\u001b[39m:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/core/indexes/base.py:6200\u001b[0m, in \u001b[0;36mIndex._get_indexer_strict\u001b[0;34m(self, key, axis_name)\u001b[0m\n\u001b[1;32m   6197\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   6198\u001b[0m     keyarr, indexer, new_indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reindex_non_unique(keyarr)\n\u001b[0;32m-> 6200\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_raise_if_missing\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkeyarr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   6202\u001b[0m keyarr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtake(indexer)\n\u001b[1;32m   6203\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, Index):\n\u001b[1;32m   6204\u001b[0m     \u001b[38;5;66;03m# GH 42790 - Preserve name from an Index\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/core/indexes/base.py:6252\u001b[0m, in \u001b[0;36mIndex._raise_if_missing\u001b[0;34m(self, key, indexer, axis_name)\u001b[0m\n\u001b[1;32m   6249\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNone of [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m] are in the [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00maxis_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   6251\u001b[0m not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(ensure_index(key)[missing_mask\u001b[38;5;241m.\u001b[39mnonzero()[\u001b[38;5;241m0\u001b[39m]]\u001b[38;5;241m.\u001b[39munique())\n\u001b[0;32m-> 6252\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnot_found\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not in index\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mKeyError\u001b[0m: \"['deg', 'closeness', 'betweenness', 'n'] not in index\""
     ]
    }
   ],
   "source": [
    "model_lr = LogisticRegression(class_weight='balanced')\n",
    "# model_lr_adv = LogisticRegression(penalty = 'l2', \n",
    "#                                   solver = 'saga',   # Sparse features: have mostly 0-values\n",
    "#                                   C=1.0, \n",
    "#                                   class_weight='balanced',\n",
    "#                                   max_iter=1000,\n",
    "#                                   random_state=42)\n",
    "\n",
    "#features = ['deg', 'closeness', 'betweenness', 'pagerank']\n",
    "features_n = ['deg', 'closeness', 'betweenness', 'pagerank', 'n']\n",
    "#features_add = ['deg', 'closeness', 'betweenness', 'pagerank', 'n','eigenvector', 'katz']\n",
    "\n",
    "#acc_lr, prec_lr, recall_lr, f1_lr = GKFold_training(model_lr, features, df_normalized)\n",
    "acc_lr, prec_lr, recall_lr, f1_lr = GKFold_training(model_lr, features_n, df_train)\n",
    "#acc_lr, prec_lr, recall_lr, f1_lr = GKFold_training(model_lr_adv, features_n, df_normalized)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19f54ec8",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b36e2dbe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
