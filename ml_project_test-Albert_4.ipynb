{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "324e8a16",
   "metadata": {},
   "source": [
    "# üêç Machine Learning Project Script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "183be9d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import ast\n",
    "import networkx as nx\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efd12ce1",
   "metadata": {},
   "source": [
    "# 1. Upload data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "5d76f234",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_raw = pd.read_csv(\"train.csv\", sep=',')\n",
    "df_train_raw[\"edgelist\"].head(1)\n",
    "df_train_raw[\"edgelist\"] = df_train_raw[\"edgelist\"].apply(ast.literal_eval)\n",
    "\n",
    "df_test_raw = pd.read_csv(\"test.csv\", sep=',')\n",
    "df_test_raw[\"edgelist\"].head(1)\n",
    "df_test_raw[\"edgelist\"] = df_test_raw[\"edgelist\"].apply(ast.literal_eval)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e536ab68",
   "metadata": {},
   "source": [
    "# 2. Pre-Processing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "a4dd84fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import numpy as np\n",
    "from itertools import combinations\n",
    "from community import community_louvain  # pip install python-louvain\n",
    "\n",
    "def normalize_group(df_group):\n",
    "    numeric_cols = [\n",
    "        'degree', 'closeness', 'betweenness', 'pagerank',\n",
    "        'eigenvector', 'katz', 'load',\n",
    "        'eccentricity', 'avg_neighbor_degree',\n",
    "         'community', 'is_leaf'\n",
    "        #'shortest_path_length', 'is_leaf', 'neighbor_connectivity'\n",
    "    ]\n",
    "    scaler = MinMaxScaler()\n",
    "    df_group[numeric_cols] = scaler.fit_transform(df_group[numeric_cols])\n",
    "    return df_group\n",
    "\n",
    "def pre_processing(data):\n",
    "    training_data = []\n",
    "\n",
    "    for idx, row in data.iterrows():\n",
    "        edgelist = row[\"edgelist\"]\n",
    "        \n",
    "        # Create undirected graph\n",
    "        T = nx.Graph()\n",
    "        T.add_edges_from(edgelist)\n",
    "\n",
    "        if not nx.is_connected(T):\n",
    "            continue\n",
    "        \n",
    "        root_node = row.get(\"root\", None)\n",
    "        \n",
    "        # Compute centralities\n",
    "        closeness = nx.closeness_centrality(T)\n",
    "        betweenness = nx.betweenness_centrality(T)\n",
    "        pagerank = nx.pagerank(T, max_iter=1000)\n",
    "        \n",
    "        # Additional centrality measures with fallbacks\n",
    "        try:\n",
    "            eigenvector = nx.eigenvector_centrality(T, max_iter=10000, tol=1e-06)\n",
    "        except nx.PowerIterationFailedConvergence:\n",
    "            eigenvector = {n: 0.0 for n in T.nodes}\n",
    "            \n",
    "        try:\n",
    "            katz = nx.katz_centrality(T, alpha=0.1)\n",
    "        except nx.NetworkXException:\n",
    "            katz = {n: 0.0 for n in T.nodes}\n",
    "            \n",
    "        try:\n",
    "            load = nx.load_centrality(T)\n",
    "        except:\n",
    "            load = {n: 0.0 for n in T.nodes}\n",
    "\n",
    "        # Structural properties\n",
    "        degree = dict(T.degree())\n",
    "        eccentricity = nx.eccentricity(T)\n",
    "        avg_neighbor_degree = nx.average_neighbor_degree(T)\n",
    "        \n",
    "        # Community detection\n",
    "        partition = community_louvain.best_partition(T)\n",
    "        \n",
    "        for v in T.nodes:\n",
    "            features = {\n",
    "                \"sentence\": row[\"sentence\"],\n",
    "                \"language\": row[\"language\"],\n",
    "                \"n\": row[\"n\"],\n",
    "                \"node\": v,\n",
    "\n",
    "                # Centrality measures\n",
    "                \"degree\": degree[v],\n",
    "                \"closeness\": closeness[v],\n",
    "                \"betweenness\": betweenness[v],\n",
    "                \"pagerank\": pagerank[v],\n",
    "                \"eigenvector\": eigenvector[v],\n",
    "                \"katz\": katz[v],\n",
    "                \"load\": load[v],\n",
    "\n",
    "                # Structural properties\n",
    "                \"eccentricity\": eccentricity[v],\n",
    "                \"avg_neighbor_degree\": avg_neighbor_degree[v],\n",
    "\n",
    "                # Community information\n",
    "                \"community\": partition[v],\n",
    "\n",
    "                \"is_leaf\": 1 if T.degree(v) == 1 else 0,\n",
    "            }\n",
    "\n",
    "            if \"id\" in row:\n",
    "                features[\"id\"] = row[\"id\"]\n",
    "\n",
    "            if root_node is not None:\n",
    "                features[\"is_root\"] = 1 if v == root_node else 0\n",
    "\n",
    "            training_data.append(features)\n",
    "\n",
    "    training_data = pd.DataFrame(training_data)\n",
    "    \n",
    "    # Normalize features by group\n",
    "    df_normalized = training_data.groupby([\"sentence\", \"language\"], group_keys=True).apply(\n",
    "        normalize_group, include_groups=False\n",
    "    )\n",
    "    df_normalized.reset_index(inplace=True)\n",
    "    df_normalized.drop(columns=[\"level_2\"], inplace=True)\n",
    "\n",
    "    return df_normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "80041901",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>language</th>\n",
       "      <th>n</th>\n",
       "      <th>node</th>\n",
       "      <th>degree</th>\n",
       "      <th>closeness</th>\n",
       "      <th>betweenness</th>\n",
       "      <th>pagerank</th>\n",
       "      <th>eigenvector</th>\n",
       "      <th>katz</th>\n",
       "      <th>load</th>\n",
       "      <th>eccentricity</th>\n",
       "      <th>avg_neighbor_degree</th>\n",
       "      <th>community</th>\n",
       "      <th>is_leaf</th>\n",
       "      <th>is_root</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>Arabic</td>\n",
       "      <td>21</td>\n",
       "      <td>10</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.730183</td>\n",
       "      <td>0.724771</td>\n",
       "      <td>0.932971</td>\n",
       "      <td>0.990346</td>\n",
       "      <td>0.996388</td>\n",
       "      <td>0.724771</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>0.555556</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Arabic</td>\n",
       "      <td>21</td>\n",
       "      <td>8</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.908084</td>\n",
       "      <td>0.990826</td>\n",
       "      <td>0.891309</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.990826</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>0.555556</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Arabic</td>\n",
       "      <td>21</td>\n",
       "      <td>5</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.598665</td>\n",
       "      <td>0.174312</td>\n",
       "      <td>0.518343</td>\n",
       "      <td>0.547862</td>\n",
       "      <td>0.477072</td>\n",
       "      <td>0.174312</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>Arabic</td>\n",
       "      <td>21</td>\n",
       "      <td>13</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.356589</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.042182</td>\n",
       "      <td>0.236520</td>\n",
       "      <td>0.004953</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.714286</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>Arabic</td>\n",
       "      <td>21</td>\n",
       "      <td>6</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.976170</td>\n",
       "      <td>0.908257</td>\n",
       "      <td>0.415764</td>\n",
       "      <td>0.703950</td>\n",
       "      <td>0.567473</td>\n",
       "      <td>0.908257</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197474</th>\n",
       "      <td>995</td>\n",
       "      <td>Turkish</td>\n",
       "      <td>16</td>\n",
       "      <td>14</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.356543</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.277926</td>\n",
       "      <td>0.417002</td>\n",
       "      <td>0.302740</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.411765</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197475</th>\n",
       "      <td>995</td>\n",
       "      <td>Turkish</td>\n",
       "      <td>16</td>\n",
       "      <td>10</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.061625</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.029047</td>\n",
       "      <td>0.103774</td>\n",
       "      <td>0.004653</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.117647</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197476</th>\n",
       "      <td>995</td>\n",
       "      <td>Turkish</td>\n",
       "      <td>16</td>\n",
       "      <td>2</td>\n",
       "      <td>0.50</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.885714</td>\n",
       "      <td>0.467346</td>\n",
       "      <td>0.842910</td>\n",
       "      <td>0.614483</td>\n",
       "      <td>0.885714</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.509804</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197477</th>\n",
       "      <td>995</td>\n",
       "      <td>Turkish</td>\n",
       "      <td>16</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.304498</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.007093</td>\n",
       "      <td>0.335177</td>\n",
       "      <td>0.074379</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197478</th>\n",
       "      <td>995</td>\n",
       "      <td>Turkish</td>\n",
       "      <td>16</td>\n",
       "      <td>15</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.413016</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002320</td>\n",
       "      <td>0.272827</td>\n",
       "      <td>0.035827</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.411765</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>197479 rows √ó 16 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        sentence language   n  node  degree  closeness  betweenness  pagerank  \\\n",
       "0              2   Arabic  21    10    1.00   0.730183     0.724771  0.932971   \n",
       "1              2   Arabic  21     8    1.00   0.908084     0.990826  0.891309   \n",
       "2              2   Arabic  21     5    0.50   0.598665     0.174312  0.518343   \n",
       "3              2   Arabic  21    13    0.00   0.356589     0.000000  0.042182   \n",
       "4              2   Arabic  21     6    0.50   0.976170     0.908257  0.415764   \n",
       "...          ...      ...  ..   ...     ...        ...          ...       ...   \n",
       "197474       995  Turkish  16    14    0.25   0.356543     0.200000  0.277926   \n",
       "197475       995  Turkish  16    10    0.00   0.061625     0.000000  0.029047   \n",
       "197476       995  Turkish  16     2    0.50   1.000000     0.885714  0.467346   \n",
       "197477       995  Turkish  16     1    0.00   0.304498     0.000000  0.007093   \n",
       "197478       995  Turkish  16    15    0.00   0.413016     0.000000  0.002320   \n",
       "\n",
       "        eigenvector      katz      load  eccentricity  avg_neighbor_degree  \\\n",
       "0          0.990346  0.996388  0.724771      0.571429             0.555556   \n",
       "1          1.000000  1.000000  0.990826      0.428571             0.555556   \n",
       "2          0.547862  0.477072  0.174312      0.571429             0.333333   \n",
       "3          0.236520  0.004953  0.000000      0.714286             0.333333   \n",
       "4          0.703950  0.567473  0.908257      0.285714             1.000000   \n",
       "...             ...       ...       ...           ...                  ...   \n",
       "197474     0.417002  0.302740  0.200000      0.666667             0.411765   \n",
       "197475     0.103774  0.004653  0.000000      1.000000             0.117647   \n",
       "197476     0.842910  0.614483  0.885714      0.000000             0.509804   \n",
       "197477     0.335177  0.074379  0.000000      0.666667             1.000000   \n",
       "197478     0.272827  0.035827  0.000000      0.333333             0.411765   \n",
       "\n",
       "        community  is_leaf  is_root  \n",
       "0        0.000000      0.0        1  \n",
       "1        0.250000      0.0        0  \n",
       "2        0.250000      0.0        0  \n",
       "3        0.250000      1.0        0  \n",
       "4        0.250000      0.0        0  \n",
       "...           ...      ...      ...  \n",
       "197474   0.000000      0.0        0  \n",
       "197475   0.000000      1.0        0  \n",
       "197476   0.666667      0.0        0  \n",
       "197477   0.000000      1.0        0  \n",
       "197478   0.666667      1.0        0  \n",
       "\n",
       "[197479 rows x 16 columns]"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train = pre_processing(df_train_raw)\n",
    "\n",
    "df_train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e820604",
   "metadata": {},
   "source": [
    "# 3. Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "677bab87",
   "metadata": {},
   "source": [
    "**K-Fold Cross Validation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "678963a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedGroupKFold\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "\n",
    "def enhanced_training_pipeline(df, features, n_folds=20):\n",
    "    df['group_id'] = df[\"sentence\"].astype(str) + '_' + df[\"language\"]\n",
    "    X = df[features]\n",
    "    y = df['is_root']\n",
    "    groups = df[\"group_id\"]\n",
    "    \n",
    "    # Initialize models\n",
    "    models = {\n",
    "        \"Random Forest\": RandomForestClassifier(\n",
    "            max_depth=20,                # limit tree depth\n",
    "            min_samples_split=10,        # require more samples to split\n",
    "            min_samples_leaf=5,          # ensure leaves have enough data\n",
    "            n_estimators=100,            # avoid too many trees\n",
    "            class_weight='balanced',\n",
    "            random_state=42\n",
    "        ),\n",
    "        \"Decision Tree\": DecisionTreeClassifier(\n",
    "            class_weight='balanced',\n",
    "            max_depth=100,\n",
    "            min_samples_split=10,\n",
    "            random_state=42\n",
    "        ),\n",
    "        \"XVG Classifier\": XGBClassifier(\n",
    "                max_depth=4,\n",
    "                learning_rate=0.1,\n",
    "                subsample=0.8,               # row sampling\n",
    "                colsample_bytree=0.8,        # feature sampling\n",
    "                n_estimators=100,\n",
    "                reg_alpha=0.5,               # L1 regularization\n",
    "                reg_lambda=1.0,              # L2 regularization\n",
    "                eval_metric='logloss',\n",
    "                random_state=42\n",
    "        )\n",
    "\n",
    "        #\"LightGBM\": LGBMClassifier(class_weight='balanced', random_state=42),\n",
    "        #\"CatBoost\": CatBoostClassifier(verbose=0, random_state=42),\n",
    "        #\"Logistic Regression\": LogisticRegression(class_weight='balanced', max_iter=1000, random_state=42),\n",
    "        #\"SVM (RBF)\": SVC(probability=True, class_weight='balanced', kernel='rbf', random_state=42),\n",
    "        #\"MLP\": MLPClassifier(hidden_layer_sizes=(100,), activation = 'logistic', max_iter=500, random_state=42),\n",
    "        #\"Dummy\": DummyClassifier(strategy=\"most_frequent\")\n",
    "    }\n",
    "    results = {}\n",
    "\n",
    "    for model_name, model in models.items():\n",
    "        print(f\"\\n=== Evaluating {model_name} ===\")\n",
    "        metrics = {'precision': [], 'recall': [], 'f1': [], 'support': []}\n",
    "        \n",
    "        cv = StratifiedGroupKFold(n_splits=n_folds)\n",
    "        \n",
    "        for fold, (train_idx, val_idx) in enumerate(cv.split(X, y, groups=groups)):\n",
    "            X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]\n",
    "            y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]\n",
    "            \n",
    "            selector = SelectFromModel(RandomForestClassifier(n_estimators=100, random_state=42))\n",
    "            selector.fit(X_train, y_train)\n",
    "            X_train_fs = selector.transform(X_train)\n",
    "            X_val_fs = selector.transform(X_val)\n",
    "\n",
    "            \n",
    "            # Fit model\n",
    "            if model_name == \"XGBoost\":\n",
    "                model.fit(\n",
    "                    X_train_fs, y_train,\n",
    "                    eval_set=[(X_val_fs, y_val)],\n",
    "                    early_stopping_rounds=10,\n",
    "                    verbose=False\n",
    "                )\n",
    "            else:\n",
    "                model.fit(X_train_fs, y_train)\n",
    "            \n",
    "            # Evaluate\n",
    "            y_pred = model.predict(X_val_fs)\n",
    "            report = classification_report(y_val, y_pred, output_dict=True)\n",
    "            metrics['precision'].append(report['weighted avg']['precision'])\n",
    "            metrics['recall'].append(report['weighted avg']['recall'])\n",
    "            metrics['f1'].append(report['weighted avg']['f1-score'])\n",
    "            metrics['support'].append(report['weighted avg']['support'])\n",
    "        \n",
    "        # Store results\n",
    "        results[model_name] = {\n",
    "            'precision': np.mean(metrics['precision']),\n",
    "            'recall': np.mean(metrics['recall']),\n",
    "            'f1': np.mean(metrics['f1']),\n",
    "            'support': np.mean(metrics['support'])\n",
    "        }\n",
    "        \n",
    "        print(f\"\\nSimple Evaluation:\")\n",
    "        print(f\"Precision: {results[model_name]['precision']:.4f}\")\n",
    "        print(f\"Recall:    {results[model_name]['recall']:.4f}\")\n",
    "        print(f\"F1 Score:  {results[model_name]['f1']:.4f}\")\n",
    "        \n",
    "    \n",
    "    # Final model training\n",
    "    best_model_name = max(results.items(), key=lambda x: x[1]['f1'])[0]\n",
    "    print(f\"\\nBest model: {best_model_name}\")\n",
    "    best_model = models[best_model_name]\n",
    "    X_fs = selector.transform(X)\n",
    "    best_model.fit(X_fs, y)\n",
    "    \n",
    "    return results, best_model_name, best_model, selector\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "51fd11af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Evaluating Random Forest ===\n",
      "\n",
      "Simple Evaluation:\n",
      "Precision: 0.9352\n",
      "Recall:    0.9034\n",
      "F1 Score:  0.9168\n",
      "\n",
      "=== Evaluating XVG Classifier ===\n",
      "\n",
      "Simple Evaluation:\n",
      "Precision: 0.9304\n",
      "Recall:    0.9478\n",
      "F1 Score:  0.9295\n",
      "\n",
      "Best model: XVG Classifier\n"
     ]
    }
   ],
   "source": [
    "# === Run pipeline ===\n",
    "\n",
    "features = [col for col in df_train.columns if col not in ['id', 'sentence', 'language', 'is_root', 'group_id']]\n",
    "\n",
    "results, best_model_name, best_model, selector = enhanced_training_pipeline(df_train, features)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19f54ec8",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "995829e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process test data\n",
    "df_test_processed = pre_processing(df_test_raw.copy())\n",
    "X_test = df_test_processed[features]\n",
    "\n",
    "# Apply feature selection\n",
    "X_test_fs = selector.transform(X_test)\n",
    "\n",
    "# Predict probabilities\n",
    "df_test_processed['pred_proba'] = best_model.predict_proba(X_test_fs)[:, 1]\n",
    "\n",
    "# Predict root node per group (max prob)\n",
    "predicted_roots = df_test_processed.loc[\n",
    "    df_test_processed.groupby('id')['pred_proba'].idxmax()\n",
    "]\n",
    "\n",
    "# Format output\n",
    "output_df = predicted_roots[['id', 'node']].rename(columns={'node': 'root'})\n",
    "output_df = output_df.sort_values('id').reset_index(drop=True)\n",
    "output_df.to_csv('predicted_roots.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
